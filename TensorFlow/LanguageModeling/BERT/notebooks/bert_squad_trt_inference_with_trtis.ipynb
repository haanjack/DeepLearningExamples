{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Copyright 2019 NVIDIA Corporation. All Rights Reserved.\n",
    "#\n",
    "# Licensed under the Apache License, Version 2.0 (the \"License\");\n",
    "# you may not use this file except in compliance with the License.\n",
    "# You may obtain a copy of the License at\n",
    "#\n",
    "#     http://www.apache.org/licenses/LICENSE-2.0\n",
    "#\n",
    "# Unless required by applicable law or agreed to in writing, software\n",
    "# distributed under the License is distributed on an \"AS IS\" BASIS,\n",
    "# WITHOUT WARRANTIES OR CONDITIONS OF ANY KIND, either express or implied.\n",
    "# See the License for the specific language governing permissions and\n",
    "# limitations under the License.\n",
    "# =============================================================================="
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 90px; float: right;\">\n",
    "\n",
    "# End-to-End BERT (Inference)\n",
    "\n",
    "이 문서는 TensorFlow BERT pretrained weight를 TensorRT engine으로 변환한 이후, TRTIS에 연동해서 Inference Serving하는 방법을 안내하기 위해 작성이 되었습니다. 이 문서에서 처리하는 절차는 크게 다음 두 가지 입니다.\n",
    "\n",
    "**1. BERT TensorRT inference engine build**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/akamai/deeplearning/tensorrt/trt-info.png\" width=\"600\" />\n",
    "TensorFlow이용하여 학습된 BERT pretrained weight를 TensorRT engine 파일로 변환합니다. 이 예제에서는 이 과정에서 필요한 plugin 들을 build하는 과정도 포함합니다. TensorRT를 이용한 BERT Sample에 대한 자세한 설명을 보시려면, [Real-Time Natural Language Understanding with BERT Using TensorRT](https://devblogs.nvidia.com/nlu-with-tensorrt-bert/)를 참고하세요.\n",
    "\n",
    "**2. TRTIS model repository 구성 및 TRTIS 서버 실행**\n",
    "<img src=\"https://developer.nvidia.com/sites/default/files/pictures/2018/trt-inference-server-diagram-1200px.png\" width=\"600\" />\n",
    "이 예제에서는 완성된 engine 파일을 이용하여, TRTIS용 Model repository를 구성하고, TensorRT Inference Server를 실제로 구동하여 동작하는 것을 살펴볼 것입니다. TensorRT Inference Server에 대한 자세한 설명을 보시려면, [NVIDIA TensorRT Inference Server Boosts Deep Learning Inference](https://devblogs.nvidia.com/nvidia-serves-deep-learning-inference/) 문서를 참고하세요.\n",
    "\n",
    "이 문서는 DGX-1 V100 16GB 장비를 이용하여 테스트 되었으며, 구동 환경에 따라 성능은 다를 수 있습니다. 또한 사용된 SW는 다음과 같습니다.\n",
    "* CUDA 10.1 / CUDNN 7 / TensorRT 6.0\n",
    "* NGC Containers\n",
    "\n",
    "| 용도 | NGC container |\n",
    "|:---:|:---:|\n",
    "| TensorRT engine build | cuda:10.1-cudnn7-devel-ubuntu18.04 |\n",
    "| TensorRT Inference Server | tensorrtserver:12.10-py3 |\n",
    "| BERT inference client | tensorflow:12.08-py3 |\n",
    "    \n",
    "* docker / nvidia-docker2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## I. Building BERT TensorRT Inference Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Build TensorRT Docker Container\n",
    "\n",
    "TensorRT engine을 build하기 위해 우선 build 환경을 구축하기 위한 docker image를 생성합니다. 여기서는 TensorRT 6.0 BERT inference 예제를 이용할 것이며, 기준 환경인 CUDA 10.1 / TensorRT 6.0을 이용할 것입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "%%bash\n",
    "cd ../trt\n",
    "docker build . -f Dockerfile -t bert_trt --rm \\\n",
    "    --build-arg FROM_IMAGE_NAME=nvcr.io/nvidia/cuda:10.1-cudnn7-devel-ubuntu18.04 \\\n",
    "    --build-arg TRT_PKG_VERSION=6.0.1-1+cuda10.1 \\\n",
    "    --build-arg myuid=$(id -u) --build-arg mygid=$(id -g) > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 문서에서는 편의상 docker image를 build 하면서 생기는 log를 숨겼습니다. 자세한 로그를 확인하고 싶으시다면 위 코드에서 ```> /dev/null 2>&1```을 제거하고 코드를 실행하시면 됩니다.\n",
    "\n",
    "한편 이 이미지를 빌드하는 과정에서 TensorRT 6.0의 repository를 clone하여, BERT TensorRT plugin layer들을 공유 라이브러리 파일 형태로 구성합니다. 이 과정을 위해 Dockerfile에 아래의 코드가 삽입되어 있습니다. \n",
    "\n",
    "```bash\n",
    "RUN git clone -b release/6.0 https://github.com/nvidia/TensorRT TensorRT && \\\n",
    "   cd TensorRT && \\\n",
    "   git submodule update --init --recursive && \\\n",
    "   mkdir -p demo/BERT/build && cd demo/BERT/build && \\\n",
    "   cmake .. && \\\n",
    "   make -j$(nproc)\n",
    "```\n",
    "\n",
    "그리고 이렇게 build한 라이브러리를 TensorRT Inference Server에서 가져다 사용하기 위해 아래의 명령어로 압축을 합니다.\n",
    "\n",
    "```bash\n",
    "tar -czf bert_plugin.tar.gz libbert_plugins.so libcommon.so\n",
    "```\n",
    "\n",
    "자세한 내용은 [```trt/Dockerfile```](../trt/Dockerfile)을 참고해주시기 바랍니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Container 실행"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음 docker 실행 명령을 이용하여 BERT TensorRT engine을 build하기 위한 container를 실행합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "c14963031837e80d0b4c557b8aa4c70a1148551dac9341dd58fc12c07219ff9b\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Error: No such container: bert_trt\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "cd ..\n",
    "\n",
    "GPU_ID=${1:-\"all\"}\n",
    "\n",
    "ENGINE_OUTPUT_DIR=\"outputs\"\n",
    "\n",
    "if [[ ! -e ${ENGINE_OUTPUT_DIR} ]]; then\n",
    "    mkdir -p ${ENGINE_OUTPUT_DIR}\n",
    "fi\n",
    "\n",
    "docker rm -f bert_trt\n",
    "docker run -d -ti \\\n",
    "    --name bert_trt${VERSION} \\\n",
    "    --gpus ${GPU_ID} \\\n",
    "    --shm-size=1g --ulimit memlock=1 --ulimit stack=67108864 \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd)/outputs:/workspace/outputs \\\n",
    "    -v $(pwd)/results/models:/workspace/models \\\n",
    "    bert_trt bash"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CONTAINER ID        IMAGE               COMMAND             CREATED             STATUS                  PORTS               NAMES\r\n",
      "c14963031837        bert_trt            \"bash\"              1 second ago        Up Less than a second                       bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker ps -a"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "앞에서 docker image build 과정에서 설명한 것과 같이, ```/workspace/TensorRT/demo/BERT/build```경로에는 BERT TensorRT plugin이 빌드되어 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CMakeCache.txt\tMakefile\t    cmake_install.cmake  libcommon.so\r",
      "\r\n",
      "CMakeFiles\tbert_plugin.tar.gz  libbert_plugins.so\t sample_bert\r",
      "\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -t bert_trt ls /workspace/TensorRT/demo/BERT/build"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Build TensorRT Plugin Layer library and download pretrained weights"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "BERT TensorRT engine을 build하기 위해 필요한 plugin의 라이브러리와 예제를 위해 pretrained-weight를 ngc로부터 다운로드 받습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. Pre-trained weight download\n",
    "NGC로부터 pre-trained weight를 다운로드 받습니다. NGC에서는 다음의 조건에 대한 pretrained weight를 제공하므로 이 중에 선택하여 사용할 수 있습니다.\n",
    "\n",
    "| | options |\n",
    "|:---:|:---:|\n",
    "| model | large, base |\n",
    "| precision | fp32, fp16 |\n",
    "| seq. length | 128, 384 |\n",
    "\n",
    "물론 독자적으로 학습하신 weight (ckpt)를 사용하실 수도 있습니다.\n",
    "\n",
    "이 예제에서는 bert-large, fp16, seq-len 128 을 사용하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash -s 'base' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "if [[ ! -d ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/ ]]; then\n",
    "    docker exec -t bert_trt bash /workspace/TensorRT/demo/BERT/python/download_fine-tuned_model.sh ${MODEL} ${FT_PRECISION} ${SEQ_LEN}\n",
    "fi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "NGC에서 다운로드 받은 pretrained weight는 ```results/models/fine-tuned/```에 저장이 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_config.json\n",
      "model.ckpt-8144.data-00000-of-00001\n",
      "model.ckpt-8144.index\n",
      "model.ckpt-8144.meta\n",
      "tf_bert_squad_1n_fp16_gbs32.190523114449.log\n",
      "vocab.txt\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'base' 'fp16' '128'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "\n",
    "ls ../results/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Build TensorRT Engine"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 BERT TensorRT precision을 build할 것입니다. 이전에 NGC model repository에서 다운로드 받은 pretrained weight와 동일한 조건으로 engine을 build하되 batch size를 지정해 줘야 합니다. 현재 TensorRT Inference Server와 호환성 문제로 batch size 1까지만을 지원하므로, 여기서는 batch size를 1로 설정해서 engine을 빌드하도록 하겠습니다.\n",
    "\n",
    "참고로 별도의 터미널 창을 열어서 ```watch -n1 nvidia-smi```를 이용하면, TensorRT가 engine을 Build하면서 GPU를 점유하는 것을 보실 수 있습니다. 이 과정에서 GPU를 사용하는 이유는 engine을 build하는 과정에서 target GPU의 성능을 측정하여 적절한 GPU Kernel의 구성을 TensorRT가 찾기 때문입니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[TensorRT] INFO: Using configuration file: /workspace/models/fine-tuned/bert_tf_v2_base_fp16_128_v2/bert_config.json\n",
      "[TensorRT] INFO: Found 202 entries in weight map\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Detected 3 inputs and 1 output network tensors.\n",
      "[TensorRT] INFO: Saving Engine to /workspace/outputs/bert_base_128.engine\n",
      "[TensorRT] INFO: Done.\n",
      "CPU times: user 20 ms, sys: 8.5 ms, total: 28.5 ms\n",
      "Wall time: 7min 58s\n"
     ]
    }
   ],
   "source": [
    "%%time\n",
    "%%bash -s 'base' 'fp16' '128' '1' \n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_builder.py \\\n",
    "            -m /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/model.ckpt-8144 \\\n",
    "            -c /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2 \\\n",
    "            -o /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine \\\n",
    "            -s ${SEQ_LEN} -b ${BATCH_SIZE}"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "위 Script의 실행결과 아래 경로에 engine 파일이 생성된 것을 볼 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_base_128.engine\r\n"
     ]
    }
   ],
   "source": [
    "!docker exec -ti bert_trt ls /workspace/outputs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Inference Test"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 build 한 TensorRT engine을 이용해서 inference를 테스트해보겠습니다.\n",
    "\n",
    "질문의 내용을 바꿔가면서 답변이 바뀌는 것들을 보실 수 있습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 1. Introductin TensorRT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Passage: TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\r\n",
      "WARNING:tensorflow:From /workspace/TensorRT/demo/BERT/python/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "\r\n",
      "Question: What is TensorRT?\r\n",
      "\r\n",
      "Running Inference...\r\n",
      "------------------------\r\n",
      "Running inference in 214.017 Sentences/Sec\r\n",
      "------------------------\r\n",
      "Processing output 0 in batch\r\n",
      "Answer: 'high performance deep learning inference platform'\r\n",
      "With probability: 29.250\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'base' 'fp16' '128' '1'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "paragraph_text=\"TensorRT is a high performance deep learning inference platform that delivers low latency and high throughput for apps such as recommenders, speech and image/video on NVIDIA GPUs. It includes parsers to import models, and plugins to support novel ops and layers before applying optimizations for inference. Today NVIDIA is open sourcing parsers and plugins in TensorRT so that the deep learning community can customize and extend these components to take advantage of powerful TensorRT optimizations for your apps.\"\n",
    "question_text=\"What is TensorRT?\"\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_inference.py \\\n",
    "            -e /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine -s ${SEQ_LEN} \\\n",
    "            -p ${paragraph_text} \\\n",
    "            -q ${question_text} \\\n",
    "            -v /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Example 2. Apollo program"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\r\n",
      "Passage: The Apollo program was the third United States human spaceflight program. First conceived as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was dedicated to President John F. Kennedy's national goal of landing a man on the Moon. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972 followed by the Apollo-Soyuz Test Project a joint Earth orbit mission with the Soviet Union in 1975.\r\n",
      "WARNING:tensorflow:From /workspace/TensorRT/demo/BERT/python/tokenization.py:125: The name tf.gfile.GFile is deprecated. Please use tf.io.gfile.GFile instead.\r\n",
      "\r\n",
      "\r\n",
      "Question: What project put the first Americans into space?\r\n",
      "\r\n",
      "Running Inference...\r\n",
      "------------------------\r\n",
      "Running inference in 213.147 Sentences/Sec\r\n",
      "------------------------\r\n",
      "Processing output 0 in batch\r\n",
      "Answer: 'Project Mercury'\r\n",
      "With probability: 55.754\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s 'base' 'fp16' '128' '1'\n",
    "\n",
    "MODEL=${1:-'base'}\n",
    "FT_PRECISION=${2:-'fp16'}\n",
    "SEQ_LEN=${3:-'128'}\n",
    "BATCH_SIZE=${4:-'1'}\n",
    "\n",
    "paragraph_text=\"The Apollo program was the third United States human spaceflight program. First conceived as a three-man spacecraft to follow the one-man Project Mercury which put the first Americans in space, Apollo was dedicated to President John F. Kennedy's national goal of landing a man on the Moon. The first manned flight of Apollo was in 1968. Apollo ran from 1961 to 1972 followed by the Apollo-Soyuz Test Project a joint Earth orbit mission with the Soviet Union in 1975.\"\n",
    "question_text=\"What project put the first Americans into space?\"\n",
    "#question_text=\"What year did the first manned Apollo flight occur?\"\n",
    "#question_text=\"What President is credited with the original notion of putting Americans in space?\"\n",
    "#question_text=\"Who did the U.S. collaborate with on an Earth orbit mission in 1975?\"\n",
    "\n",
    "docker exec -t \\\n",
    "    bert_trt \\\n",
    "        python3 -W ignore /workspace/TensorRT/demo/BERT/python/bert_inference.py \\\n",
    "            -e /workspace/outputs/bert_${MODEL}_${SEQ_LEN}.engine -s ${SEQ_LEN} \\\n",
    "            -p ${paragraph_text} \\\n",
    "            -q ${question_text} \\\n",
    "            -v /workspace/models/fine-tuned/bert_tf_v2_${MODEL}_${FT_PRECISION}_${SEQ_LEN}_v2/vocab.txt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "예제의 체감 실행 시간과 ```bert_inference.py```에서 측정한 inference 시간에서 차이가 발생하는 이유는, 이 예제는 매번 실행할 때마다 TensorRT 엔진 파일을 Deserialize하기 때문입니다. 실제로 inference를 할 때에는, BERT Inference Engine을 GPU Instance로 띄워둔 상태로 서비스를 하게 되므로 서비스때의 속도는 측정된 시간의 속도대로 동작하게 됩니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 6. Closing the container\n",
    "\n",
    "이제 TensorRT 엔진을 빌드하는 과정을 마쳤습니다. 그럼 이제 지금까지 사용한 Container를 닫도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bert_trt\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f bert_trt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Building BERT inferencing platform with TRTIS"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. Pulling TRTIS docker image\n",
    "\n",
    "TensorRT Inference Server는 새로운 Build 없이 Serving을 할 수 있는 장점이 있습니다. 우선 원활한 예제의 실행을 위해 사용할 이미지를 다음 명령을 이용하여 pull 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker pull nvcr.io/nvidia/tensorrtserver:19.10-py3 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../trtis/Dockerfile\n"
     ]
    }
   ],
   "source": [
    "%%file ../trtis/Dockerfile\n",
    "\n",
    "ARG FROM_IMAGE_NAME\n",
    "FROM ${FROM_IMAGE_NAME}\n",
    "\n",
    "ENV LIB_WORKING_DIR=\"/opt/tensorrtserver/lib\"\n",
    "COPY --from=bert_trt /workspace/TensorRT/demo/BERT/build/bert_plugin.tar.gz ${LIB_WORKING_DIR}\n",
    "RUN tar -xzvf ${LIB_WORKING_DIR}/bert_plugin.tar.gz -C ${LIB_WORKING_DIR} && \\\n",
    "    chown -R tensorrt-server:tensorrt-server ${LIB_WORKING_DIR}\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "docker build ../trtis -t tensorrtserver:bert --build-arg FROM_IMAGE_NAME=nvcr.io/nvidia/tensorrtserver:19.10-py3 > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. Setting TRTIS model repository"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "다음의 명령들을 이용하여 TRTIS model repository를 구성합니다. 여기서 숫자 1은 model version으로 원하는 버전을 설정하실 수 있으며, 향후에 inference client 단에서 원하는 버전을 지정하여 inference가 되도록 지정하실 수 있습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "mkdir -p ../results/trtis_models/bert_base_128_fp16\n",
    "mkdir -p ../results/trtis_models/bert_base_128_fp16/1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Model repository에는 model를 명시하는 ```config.pbtxt``` 파일과 TensorRT engine 파일을 ```model.plan```으로 이름을 변경하여 버전에 따라 저장을 합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_base_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 1\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash\n",
    "cp ../outputs/bert_base_128.engine ../results/trtis_models/bert_base_128_fp16/1/model.plan"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. Launch Server"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이제 TensorRT inference server를 구동시킬 차례입니다. TensorFlow Model이 FP16으로 Inference 되도록 하게 하는 한편, TensorRT engine의 dependency를 위한 경로를 설정해 줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fp16 activated!\n",
      "f0e40d345bea8340b516bb1138d6e225af53ef53ac02c3b9b996fa696bcd7356\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"fp16\"\n",
    "\n",
    "cd ..\n",
    "\n",
    "precision=${1:-\"fp16\"}\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [ \"$precision\" = \"fp16\" ] ; then\n",
    "   echo \"fp16 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=1\n",
    "else\n",
    "   echo \"fp32 activated!\"\n",
    "   export TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE=0\n",
    "fi\n",
    "\n",
    "# Start TRTIS server in detached state\n",
    "docker run -d --rm \\\n",
    "   --gpus ${NV_VISIBLE_DEVICES} \\\n",
    "   --shm-size=1g \\\n",
    "   --ulimit memlock=-1 \\\n",
    "   --ulimit stack=67108864 \\\n",
    "   -p8000:8000 \\\n",
    "   -p8001:8001 \\\n",
    "   -p8002:8002 \\\n",
    "   --name trt_server_cont \\\n",
    "   -e TF_ENABLE_AUTO_MIXED_PRECISION_GRAPH_REWRITE \\\n",
    "   -v $(pwd)/results/trtis_models:/models \\\n",
    "   -e LD_PRELOAD=\"libcommon.so:libbert_plugins.so\" \\\n",
    "   tensorrtserver:bert \\\n",
    "        trtserver --model-store=/models --strict-model-config=false"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. Performance Test\n",
    "\n",
    "이제 구성한 TensorRT Inference Server의 성능을 확인하기 위해서 TensorRT Inference Server에서 제공하는 [perf_client](https://docs.nvidia.com/deeplearning/sdk/tensorrt-inference-server-guide/docs/perf_client.html)을 이용해서 성능을 측정해보도록 하겠습니다. 이 툴은 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1. BERT Client build\n",
    "만약 BERT Training을 위해 사용한 BERT docker image가 위치한 노드와 동일한 노드라면 다음 명령은 생략하셔도 됩니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "%%bash \n",
    "cd ..\n",
    "bash ./scripts/docker/build.sh > /dev/null 2>&1"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2. BERT TRTIS performance (1 GPU)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      "TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use 'nvidia-docker run' to start this container; see\r\n",
      "   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\r\n",
      "\r\n",
      "ERROR: Detected MOFED driver 4.7-1.0.0, but this container has version 4.4-1.0.0.\r\n",
      "       Unable to automatically upgrade this container.\r\n",
      "       Use of RDMA for multi-node communication will be unreliable.\r\n",
      "\r\n",
      "NOTE: MOFED driver was detected, but nv_peer_mem driver was not detected.\r\n",
      "      Multi-node communication performance may be reduced.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 500 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 347 infer/sec. Avg latency: 2865 usec (std 157 usec)\r\n",
      "  Pass [2] throughput: 339 infer/sec. Avg latency: 2931 usec (std 112 usec)\r\n",
      "  Pass [3] throughput: 341 infer/sec. Avg latency: 2914 usec (std 111 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1025\r\n",
      "    Throughput: 341 infer/sec\r\n",
      "    Avg latency: 2914 usec (standard deviation 111 usec)\r\n",
      "    p50 latency: 2924 usec\r\n",
      "    p90 latency: 3052 usec\r\n",
      "    p95 latency: 3090 usec\r\n",
      "    p99 latency: 3153 usec\r\n",
      "    Avg gRPC time: 2849 usec (marshal 7 usec + response wait 2832 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1230\r\n",
      "    Avg request latency: 2308 usec (overhead 12 usec + queue 47 usec + compute 2249 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 441 infer/sec. Avg latency: 4517 usec (std 96 usec)\r\n",
      "  Pass [2] throughput: 440 infer/sec. Avg latency: 4534 usec (std 93 usec)\r\n",
      "  Pass [3] throughput: 439 infer/sec. Avg latency: 4544 usec (std 96 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1317\r\n",
      "    Throughput: 439 infer/sec\r\n",
      "    Avg latency: 4544 usec (standard deviation 96 usec)\r\n",
      "    p50 latency: 4540 usec\r\n",
      "    p90 latency: 4633 usec\r\n",
      "    p95 latency: 4655 usec\r\n",
      "    p99 latency: 4709 usec\r\n",
      "    Avg gRPC time: 4483 usec (marshal 7 usec + response wait 4466 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1580\r\n",
      "    Avg request latency: 3944 usec (overhead 11 usec + queue 1715 usec + compute 2218 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 6868 usec (std 76 usec)\r\n",
      "  Pass [2] throughput: 435 infer/sec. Avg latency: 6876 usec (std 69 usec)\r\n",
      "  Pass [3] throughput: 435 infer/sec. Avg latency: 6874 usec (std 68 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1307\r\n",
      "    Throughput: 435 infer/sec\r\n",
      "    Avg latency: 6874 usec (standard deviation 68 usec)\r\n",
      "    p50 latency: 6873 usec\r\n",
      "    p90 latency: 6958 usec\r\n",
      "    p95 latency: 6986 usec\r\n",
      "    p99 latency: 7030 usec\r\n",
      "    Avg gRPC time: 6812 usec (marshal 7 usec + response wait 6795 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1568\r\n",
      "    Avg request latency: 6241 usec (overhead 12 usec + queue 3995 usec + compute 2234 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 9161 usec (std 71 usec)\r\n",
      "  Pass [2] throughput: 436 infer/sec. Avg latency: 9149 usec (std 73 usec)\r\n",
      "  Pass [3] throughput: 437 infer/sec. Avg latency: 9125 usec (std 105 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1313\r\n",
      "    Throughput: 437 infer/sec\r\n",
      "    Avg latency: 9125 usec (standard deviation 105 usec)\r\n",
      "    p50 latency: 9113 usec\r\n",
      "    p90 latency: 9225 usec\r\n",
      "    p95 latency: 9257 usec\r\n",
      "    p99 latency: 9453 usec\r\n",
      "    Avg gRPC time: 9059 usec (marshal 7 usec + response wait 9042 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1576\r\n",
      "    Avg request latency: 8473 usec (overhead 11 usec + queue 6233 usec + compute 2229 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 437 infer/sec. Avg latency: 11425 usec (std 94 usec)\r\n",
      "  Pass [2] throughput: 435 infer/sec. Avg latency: 11467 usec (std 108 usec)\r\n",
      "  Pass [3] throughput: 436 infer/sec. Avg latency: 11437 usec (std 90 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1310\r\n",
      "    Throughput: 436 infer/sec\r\n",
      "    Avg latency: 11437 usec (standard deviation 90 usec)\r\n",
      "    p50 latency: 11431 usec\r\n",
      "    p90 latency: 11552 usec\r\n",
      "    p95 latency: 11599 usec\r\n",
      "    p99 latency: 11686 usec\r\n",
      "    Avg gRPC time: 11379 usec (marshal 6 usec + response wait 11363 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1571\r\n",
      "    Avg request latency: 10823 usec (overhead 10 usec + queue 8577 usec + compute 2236 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 437 infer/sec. Avg latency: 13718 usec (std 111 usec)\r\n",
      "  Pass [2] throughput: 436 infer/sec. Avg latency: 13731 usec (std 115 usec)\r\n",
      "  Pass [3] throughput: 436 infer/sec. Avg latency: 13733 usec (std 116 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1309\r\n",
      "    Throughput: 436 infer/sec\r\n",
      "    Avg latency: 13733 usec (standard deviation 116 usec)\r\n",
      "    p50 latency: 13725 usec\r\n",
      "    p90 latency: 13864 usec\r\n",
      "    p95 latency: 13905 usec\r\n",
      "    p99 latency: 14034 usec\r\n",
      "    Avg gRPC time: 13673 usec (marshal 7 usec + response wait 13656 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1572\r\n",
      "    Avg request latency: 13084 usec (overhead 11 usec + queue 10837 usec + compute 2236 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 16029 usec (std 125 usec)\r\n",
      "  Pass [2] throughput: 435 infer/sec. Avg latency: 16089 usec (std 126 usec)\r\n",
      "  Pass [3] throughput: 436 infer/sec. Avg latency: 16033 usec (std 135 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1309\r\n",
      "    Throughput: 436 infer/sec\r\n",
      "    Avg latency: 16033 usec (standard deviation 135 usec)\r\n",
      "    p50 latency: 16018 usec\r\n",
      "    p90 latency: 16178 usec\r\n",
      "    p95 latency: 16262 usec\r\n",
      "    p99 latency: 16559 usec\r\n",
      "    Avg gRPC time: 15958 usec (marshal 7 usec + response wait 15940 usec + unmarshal 11 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1571\r\n",
      "    Avg request latency: 15360 usec (overhead 10 usec + queue 13114 usec + compute 2236 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 18299 usec (std 126 usec)\r\n",
      "  Pass [2] throughput: 436 infer/sec. Avg latency: 18313 usec (std 110 usec)\r\n",
      "  Pass [3] throughput: 436 infer/sec. Avg latency: 18316 usec (std 128 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1309\r\n",
      "    Throughput: 436 infer/sec\r\n",
      "    Avg latency: 18316 usec (standard deviation 128 usec)\r\n",
      "    p50 latency: 18306 usec\r\n",
      "    p90 latency: 18455 usec\r\n",
      "    p95 latency: 18530 usec\r\n",
      "    p99 latency: 18783 usec\r\n",
      "    Avg gRPC time: 18251 usec (marshal 7 usec + response wait 18234 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1572\r\n",
      "    Avg request latency: 17697 usec (overhead 8 usec + queue 15452 usec + compute 2237 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 436 infer/sec. Avg latency: 20612 usec (std 123 usec)\r\n",
      "  Pass [2] throughput: 434 infer/sec. Avg latency: 20691 usec (std 120 usec)\r\n",
      "  Pass [3] throughput: 433 infer/sec. Avg latency: 20755 usec (std 87 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1301\r\n",
      "    Throughput: 433 infer/sec\r\n",
      "    Avg latency: 20755 usec (standard deviation 87 usec)\r\n",
      "    p50 latency: 20754 usec\r\n",
      "    p90 latency: 20865 usec\r\n",
      "    p95 latency: 20900 usec\r\n",
      "    p99 latency: 20951 usec\r\n",
      "    Avg gRPC time: 20696 usec (marshal 7 usec + response wait 20679 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1560\r\n",
      "    Avg request latency: 20151 usec (overhead 8 usec + queue 17901 usec + compute 2242 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 433 infer/sec. Avg latency: 23070 usec (std 117 usec)\r\n",
      "  Pass [2] throughput: 433 infer/sec. Avg latency: 23069 usec (std 111 usec)\r\n",
      "  Pass [3] throughput: 433 infer/sec. Avg latency: 23075 usec (std 112 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 1300\r\n",
      "    Throughput: 433 infer/sec\r\n",
      "    Avg latency: 23075 usec (standard deviation 112 usec)\r\n",
      "    p50 latency: 23073 usec\r\n",
      "    p90 latency: 23222 usec\r\n",
      "    p95 latency: 23264 usec\r\n",
      "    p99 latency: 23321 usec\r\n",
      "    Avg gRPC time: 23007 usec (marshal 7 usec + response wait 22990 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 1560\r\n",
      "    Avg request latency: 22442 usec (overhead 10 usec + queue 20189 usec + compute 2243 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 341 infer/sec, latency 2914 usec\r\n",
      "Concurrency: 2, 439 infer/sec, latency 4544 usec\r\n",
      "Concurrency: 3, 435 infer/sec, latency 6874 usec\r\n",
      "Concurrency: 4, 437 infer/sec, latency 9125 usec\r\n",
      "Concurrency: 5, 436 infer/sec, latency 11437 usec\r\n",
      "Concurrency: 6, 436 infer/sec, latency 13733 usec\r\n",
      "Concurrency: 7, 436 infer/sec, latency 16033 usec\r\n",
      "Concurrency: 8, 436 infer/sec, latency 18316 usec\r\n",
      "Concurrency: 9, 433 infer/sec, latency 20755 usec\r\n",
      "Concurrency: 10, 433 infer/sec, latency 23075 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"base\" \"128\" \"fp16\" \"1\" \"1\" \"500\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"base\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "cd ..\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3. Model reconfiguration & updated inference performance - 2 GPU"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "GPU의 구성을 바꾸기 위해 model repository에 있는 GPU 정보를 새롭게 업데이트 해줍니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Overwriting ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n"
     ]
    }
   ],
   "source": [
    "%%file ../results/trtis_models/bert_base_128_fp16/config.pbtxt\n",
    "\n",
    "name: \"bert_base_128_fp16\"\n",
    "platform: \"tensorrt_plan\"\n",
    "max_batch_size: 1\n",
    "\n",
    "instance_group [\n",
    "    {\n",
    "        count: 0\n",
    "        kind: KIND_GPU\n",
    "        gpus: [0, 1]\n",
    "    }\n",
    "]\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "그러면 TensorRT Inference Server에서는 이를 감지하여 새로 GPU instance를 구성합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Waiting for TRTIS Server to be ready at http://localhost:8000...\n",
      ".........TRTIS Server is ready!\n"
     ]
    }
   ],
   "source": [
    "%%bash\n",
    "SERVER_URI=${1:-\"localhost\"}\n",
    "\n",
    "echo \"Waiting for TRTIS Server to be ready at http://$SERVER_URI:8000...\"\n",
    "\n",
    "live_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/live\"\n",
    "ready_command=\"curl -m 1 -L -s -o /dev/null -w %{http_code} http://$SERVER_URI:8000/api/health/ready\"\n",
    "\n",
    "current_status=$($live_command)\n",
    "\n",
    "# First check the current status. If that passes, check the json. If either fail, loop\n",
    "while [[ ${current_status} != \"200\" ]] || [[ $($ready_command) != \"200\" ]]; do\n",
    "\n",
    "   printf \".\"\n",
    "   sleep 1\n",
    "   current_status=$($live_command)\n",
    "done\n",
    "\n",
    "echo \"TRTIS Server is ready!\""
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "이 시점에서 nvidia-smi 등을 통해서 GPU 메모리 사용량을 보시면, 2개의 GPU에 메모리 사용량이 늘어난 것을 보실 수 있습니다.\n",
    "\n",
    "이제 테스트를 해볼 차례입니다. 다만 여기서는 Client 단의 max latency를 기존의 500에서 1000으로 조정하도록 하겠습니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                                                                                                                                                \r\n",
      "================\r\n",
      "== TensorFlow ==\r\n",
      "================\r\n",
      "\r\n",
      "NVIDIA Release 19.08 (build 7791926)\r\n",
      "TensorFlow Version 1.14.0\r\n",
      "\r\n",
      "Container image Copyright (c) 2019, NVIDIA CORPORATION.  All rights reserved.\r\n",
      "Copyright 2017-2019 The TensorFlow Authors.  All rights reserved.\r\n",
      "\r\n",
      "Various files include modifications (c) NVIDIA CORPORATION.  All rights reserved.\r\n",
      "NVIDIA modifications are covered by the license terms that apply to the underlying project or file.\r\n",
      "\r\n",
      "WARNING: The NVIDIA Driver was not detected.  GPU functionality will not be available.\r\n",
      "   Use 'nvidia-docker run' to start this container; see\r\n",
      "   https://github.com/NVIDIA/nvidia-docker/wiki/nvidia-docker .\r\n",
      "\r\n",
      "ERROR: Detected MOFED driver 4.7-1.0.0, but this container has version 4.4-1.0.0.\r\n",
      "       Unable to automatically upgrade this container.\r\n",
      "       Use of RDMA for multi-node communication will be unreliable.\r\n",
      "\r\n",
      "NOTE: MOFED driver was detected, but nv_peer_mem driver was not detected.\r\n",
      "      Multi-node communication performance may be reduced.\r\n",
      "\r\n",
      "*** Measurement Settings ***\r\n",
      "  Batch size: 1\r\n",
      "  Measurement window: 3000 msec\r\n",
      "  Latency limit: 1000 msec\r\n",
      "  Concurrency limit: 10 concurrent requests\r\n",
      "  Stabilizing using average latency\r\n",
      "\r\n",
      "Request concurrency: 1\r\n",
      "  Pass [1] throughput: 225 infer/sec. Avg latency: 4437 usec (std 281 usec)\r\n",
      "  Pass [2] throughput: 253 infer/sec. Avg latency: 3942 usec (std 207 usec)\r\n",
      "  Pass [3] throughput: 257 infer/sec. Avg latency: 3874 usec (std 168 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 772\r\n",
      "    Throughput: 257 infer/sec\r\n",
      "    Avg latency: 3874 usec (standard deviation 168 usec)\r\n",
      "    p50 latency: 3864 usec\r\n",
      "    p90 latency: 4002 usec\r\n",
      "    p95 latency: 4036 usec\r\n",
      "    p99 latency: 4229 usec\r\n",
      "    Avg gRPC time: 3803 usec (marshal 7 usec + response wait 3786 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 928\r\n",
      "    Avg request latency: 3254 usec (overhead 13 usec + queue 60 usec + compute 3181 usec)\r\n",
      "\r\n",
      "Request concurrency: 2\r\n",
      "  Pass [1] throughput: 680 infer/sec. Avg latency: 2926 usec (std 128 usec)\r\n",
      "  Pass [2] throughput: 676 infer/sec. Avg latency: 2943 usec (std 123 usec)\r\n",
      "  Pass [3] throughput: 676 infer/sec. Avg latency: 2944 usec (std 115 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2029\r\n",
      "    Throughput: 676 infer/sec\r\n",
      "    Avg latency: 2944 usec (standard deviation 115 usec)\r\n",
      "    p50 latency: 2939 usec\r\n",
      "    p90 latency: 3096 usec\r\n",
      "    p95 latency: 3135 usec\r\n",
      "    p99 latency: 3240 usec\r\n",
      "    Avg gRPC time: 2880 usec (marshal 6 usec + response wait 2864 usec + unmarshal 10 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 2435\r\n",
      "    Avg request latency: 2322 usec (overhead 11 usec + queue 58 usec + compute 2253 usec)\r\n",
      "\r\n",
      "Request concurrency: 3\r\n",
      "  Pass [1] throughput: 879 infer/sec. Avg latency: 3397 usec (std 310 usec)\r\n",
      "  Pass [2] throughput: 877 infer/sec. Avg latency: 3408 usec (std 368 usec)\r\n",
      "  Pass [3] throughput: 875 infer/sec. Avg latency: 3416 usec (std 301 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2625\r\n",
      "    Throughput: 875 infer/sec\r\n",
      "    Avg latency: 3416 usec (standard deviation 301 usec)\r\n",
      "    p50 latency: 3408 usec\r\n",
      "    p90 latency: 3818 usec\r\n",
      "    p95 latency: 3885 usec\r\n",
      "    p99 latency: 3994 usec\r\n",
      "    Avg gRPC time: 3355 usec (marshal 6 usec + response wait 3340 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3151\r\n",
      "    Avg request latency: 2826 usec (overhead 10 usec + queue 594 usec + compute 2222 usec)\r\n",
      "\r\n",
      "Request concurrency: 4\r\n",
      "  Pass [1] throughput: 876 infer/sec. Avg latency: 4554 usec (std 64 usec)\r\n",
      "  Pass [2] throughput: 874 infer/sec. Avg latency: 4564 usec (std 65 usec)\r\n",
      "  Pass [3] throughput: 870 infer/sec. Avg latency: 4587 usec (std 80 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2610\r\n",
      "    Throughput: 870 infer/sec\r\n",
      "    Avg latency: 4587 usec (standard deviation 80 usec)\r\n",
      "    p50 latency: 4585 usec\r\n",
      "    p90 latency: 4669 usec\r\n",
      "    p95 latency: 4698 usec\r\n",
      "    p99 latency: 4750 usec\r\n",
      "    Avg gRPC time: 4526 usec (marshal 6 usec + response wait 4511 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3135\r\n",
      "    Avg request latency: 3995 usec (overhead 13 usec + queue 1752 usec + compute 2230 usec)\r\n",
      "\r\n",
      "Request concurrency: 5\r\n",
      "  Pass [1] throughput: 872 infer/sec. Avg latency: 5720 usec (std 309 usec)\r\n",
      "  Pass [2] throughput: 869 infer/sec. Avg latency: 5737 usec (std 448 usec)\r\n",
      "  Pass [3] throughput: 869 infer/sec. Avg latency: 5737 usec (std 612 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2608\r\n",
      "    Throughput: 869 infer/sec\r\n",
      "    Avg latency: 5737 usec (standard deviation 612 usec)\r\n",
      "    p50 latency: 5718 usec\r\n",
      "    p90 latency: 6592 usec\r\n",
      "    p95 latency: 6722 usec\r\n",
      "    p99 latency: 6874 usec\r\n",
      "    Avg gRPC time: 5678 usec (marshal 7 usec + response wait 5662 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3130\r\n",
      "    Avg request latency: 5126 usec (overhead 12 usec + queue 2876 usec + compute 2238 usec)\r\n",
      "\r\n",
      "Request concurrency: 6\r\n",
      "  Pass [1] throughput: 870 infer/sec. Avg latency: 6875 usec (std 108 usec)\r\n",
      "  Pass [2] throughput: 870 infer/sec. Avg latency: 6879 usec (std 107 usec)\r\n",
      "  Pass [3] throughput: 872 infer/sec. Avg latency: 6866 usec (std 82 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2616\r\n",
      "    Throughput: 872 infer/sec\r\n",
      "    Avg latency: 6866 usec (standard deviation 82 usec)\r\n",
      "    p50 latency: 6862 usec\r\n",
      "    p90 latency: 6957 usec\r\n",
      "    p95 latency: 6987 usec\r\n",
      "    p99 latency: 7061 usec\r\n",
      "    Avg gRPC time: 6806 usec (marshal 6 usec + response wait 6791 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3140\r\n",
      "    Avg request latency: 6244 usec (overhead 12 usec + queue 4000 usec + compute 2232 usec)\r\n",
      "\r\n",
      "Request concurrency: 7\r\n",
      "  Pass [1] throughput: 874 infer/sec. Avg latency: 7997 usec (std 433 usec)\r\n",
      "  Pass [2] throughput: 868 infer/sec. Avg latency: 8046 usec (std 532 usec)\r\n",
      "  Pass [3] throughput: 867 infer/sec. Avg latency: 8058 usec (std 289 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2602\r\n",
      "    Throughput: 867 infer/sec\r\n",
      "    Avg latency: 8058 usec (standard deviation 289 usec)\r\n",
      "    p50 latency: 8052 usec\r\n",
      "    p90 latency: 8417 usec\r\n",
      "    p95 latency: 8573 usec\r\n",
      "    p99 latency: 8803 usec\r\n",
      "    Avg gRPC time: 8004 usec (marshal 6 usec + response wait 7989 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3123\r\n",
      "    Avg request latency: 7447 usec (overhead 10 usec + queue 5196 usec + compute 2241 usec)\r\n",
      "\r\n",
      "Request concurrency: 8\r\n",
      "  Pass [1] throughput: 864 infer/sec. Avg latency: 9246 usec (std 82 usec)\r\n",
      "  Pass [2] throughput: 866 infer/sec. Avg latency: 9223 usec (std 136 usec)\r\n",
      "  Pass [3] throughput: 866 infer/sec. Avg latency: 9227 usec (std 117 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2598\r\n",
      "    Throughput: 866 infer/sec\r\n",
      "    Avg latency: 9227 usec (standard deviation 117 usec)\r\n",
      "    p50 latency: 9221 usec\r\n",
      "    p90 latency: 9329 usec\r\n",
      "    p95 latency: 9365 usec\r\n",
      "    p99 latency: 9540 usec\r\n",
      "    Avg gRPC time: 9167 usec (marshal 6 usec + response wait 9152 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3117\r\n",
      "    Avg request latency: 8601 usec (overhead 11 usec + queue 6345 usec + compute 2245 usec)\r\n",
      "\r\n",
      "Request concurrency: 9\r\n",
      "  Pass [1] throughput: 868 infer/sec. Avg latency: 10356 usec (std 615 usec)\r\n",
      "  Pass [2] throughput: 871 infer/sec. Avg latency: 10320 usec (std 535 usec)\r\n",
      "  Pass [3] throughput: 870 infer/sec. Avg latency: 10326 usec (std 563 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2612\r\n",
      "    Throughput: 870 infer/sec\r\n",
      "    Avg latency: 10326 usec (standard deviation 563 usec)\r\n",
      "    p50 latency: 10316 usec\r\n",
      "    p90 latency: 11036 usec\r\n",
      "    p95 latency: 11178 usec\r\n",
      "    p99 latency: 11335 usec\r\n",
      "    Avg gRPC time: 10265 usec (marshal 6 usec + response wait 10250 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3135\r\n",
      "    Avg request latency: 9716 usec (overhead 12 usec + queue 7467 usec + compute 2237 usec)\r\n",
      "\r\n",
      "Request concurrency: 10\r\n",
      "  Pass [1] throughput: 871 infer/sec. Avg latency: 11465 usec (std 98 usec)\r\n",
      "  Pass [2] throughput: 871 infer/sec. Avg latency: 11465 usec (std 100 usec)\r\n",
      "  Pass [3] throughput: 870 infer/sec. Avg latency: 11480 usec (std 99 usec)\r\n",
      "  Client: \r\n",
      "    Request count: 2610\r\n",
      "    Throughput: 870 infer/sec\r\n",
      "    Avg latency: 11480 usec (standard deviation 99 usec)\r\n",
      "    p50 latency: 11477 usec\r\n",
      "    p90 latency: 11600 usec\r\n",
      "    p95 latency: 11635 usec\r\n",
      "    p99 latency: 11713 usec\r\n",
      "    Avg gRPC time: 11419 usec (marshal 6 usec + response wait 11404 usec + unmarshal 9 usec)\r\n",
      "  Server: \r\n",
      "    Request count: 3133\r\n",
      "    Avg request latency: 10860 usec (overhead 12 usec + queue 8613 usec + compute 2235 usec)\r\n",
      "\r\n",
      "Inferences/Second vs. Client Average Batch Latency\r\n",
      "Concurrency: 1, 257 infer/sec, latency 3874 usec\r\n",
      "Concurrency: 2, 676 infer/sec, latency 2944 usec\r\n",
      "Concurrency: 3, 875 infer/sec, latency 3416 usec\r\n",
      "Concurrency: 4, 870 infer/sec, latency 4587 usec\r\n",
      "Concurrency: 5, 869 infer/sec, latency 5737 usec\r\n",
      "Concurrency: 6, 872 infer/sec, latency 6866 usec\r\n",
      "Concurrency: 7, 867 infer/sec, latency 8058 usec\r\n",
      "Concurrency: 8, 866 infer/sec, latency 9227 usec\r\n",
      "Concurrency: 9, 870 infer/sec, latency 10326 usec\r\n",
      "Concurrency: 10, 870 infer/sec, latency 11480 usec\r\n"
     ]
    }
   ],
   "source": [
    "%%bash -s \"base\" \"128\" \"fp16\" \"1\" \"1\" \"1000\" \"10\" \"10\" \"localhost\"\n",
    "\n",
    "MODEL=${1:-\"base\"}\n",
    "SEQ_LEN=${2:-\"128\"}\n",
    "FT_PRECISION=${3:-\"fp16\"}\n",
    "BATCH_SIZE=${4:-1}\n",
    "MODEL_VERSION=${5:-1}\n",
    "MAX_LATENCY=${6:-500}\n",
    "MAX_CLIENT_THREADS=${7:-10}\n",
    "MAX_CONCURRENCY=${8:-50}\n",
    "SERVER_HOSTNAME=${9:-\"localhost\"}\n",
    "\n",
    "MODEL_NAME=\"bert_${MODEL}_${SEQ_LEN}_${FT_PRECISION}\"\n",
    "NV_VISIBLE_DEVICES=${NVIDIA_VISIBLE_DEVICES:-\"all\"}\n",
    "\n",
    "if [[ $SERVER_HOSTNAME == *\":\"* ]]; then\n",
    "  echo \"ERROR! Do not include the port when passing the Server Hostname. These scripts require that the TRTIS HTTP endpoint is on Port 8000 and the gRPC endpoint is on Port 8001. Exiting...\"\n",
    "  exit 1\n",
    "fi\n",
    "\n",
    "if [[ ! -e ../results/perf_client/${MODLE_NAME} ]]; then\n",
    "    mkdir ../results/perf_client/${MODEL_NAME}\n",
    "fi\n",
    "\n",
    "TIMESTAMP=$(date \"+%y%m%d_%H%M\")\n",
    "OUTPUT_FILE_CSV=\"/results/perf_client/${MODEL_NAME}/results_${TIMESTAMP}.csv\"\n",
    "\n",
    "cd ..\n",
    "docker run --rm -t \\\n",
    "    --net=host \\\n",
    "    --shm-size=1g \\\n",
    "    --ulimit memlock=-1 \\\n",
    "    --ulimit stack=67108864 \\\n",
    "    -e NVIDIA_VISIBLE_DEVICES=$NV_VISIBLE_DEVICES \\\n",
    "    -u $(id -u):$(id -g) \\\n",
    "    -v $(pwd):/workspace/bert \\\n",
    "    -v $(pwd)/results:/results \\\n",
    "    bert \\\n",
    "        /workspace/install/bin/perf_client \\\n",
    "            --max-threads ${MAX_CLIENT_THREADS} \\\n",
    "            -m ${MODEL_NAME} \\\n",
    "            -x ${MODEL_VERSION} \\\n",
    "            -p 3000 \\\n",
    "            -d \\\n",
    "            -v \\\n",
    "            -i gRPC \\\n",
    "            -u ${SERVER_HOSTNAME}:8001 \\\n",
    "            -b ${BATCH_SIZE} \\\n",
    "            -l ${MAX_LATENCY} \\\n",
    "            -c ${MAX_CONCURRENCY} \\\n",
    "            -f ${OUTPUT_FILE_CSV} \\\n",
    "            -z"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "축하합니다. 이제 BERT 모델을 GPU를 이용하여 최적의 성능으로 Inference를 하실 수 있게 되셨습니다."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 5. Closing TRTIS container\n",
    "\n",
    "이제 아래 명령을 통해 실행한 container를 종료합니다."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "trt_server_cont\r\n"
     ]
    }
   ],
   "source": [
    "!docker rm -f trt_server_cont"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"http://developer.download.nvidia.com/compute/machine-learning/frameworks/nvidia_logo.png\" style=\"width: 150px; float: center;\">"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
